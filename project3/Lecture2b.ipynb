{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Lecture2b.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiMwkmdLtKL",
        "colab_type": "text"
      },
      "source": [
        "Marcin Grzyb marcingrzyb@student.agh.edu.pl \n",
        "Paweł Gałka pawelgalka@student.agh.edu.pl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKIdTq91LRCW",
        "colab_type": "code",
        "outputId": "8a784d24-0b71-4078-d9fa-ca7adcefab7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2IiE_LnLRCZ",
        "colab_type": "text"
      },
      "source": [
        "# Classifying newswires: a multi-class classification example\n",
        "\n",
        "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
        "But what happens when you have more than two classes? \n",
        "\n",
        "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
        "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
        "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
        "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIqzSWfILRCa",
        "colab_type": "text"
      },
      "source": [
        "## The Reuters dataset\n",
        "\n",
        "\n",
        "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
        "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
        "topic has at least 10 examples in the training set.\n",
        "\n",
        "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZxSiuMLRCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lop-2mH8LRCd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
        "data.\n",
        "\n",
        "We have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IppYZhmGLRCe",
        "colab_type": "code",
        "outputId": "6a32e8ab-8703-491d-9e1f-c10a5657be8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_data.shape)\n",
        "len(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWK7FGHdLRCg",
        "colab_type": "code",
        "outputId": "3ad532e8-56e4-432d-ffea-8707d3f0107e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6brmqwsLRCj",
        "colab_type": "text"
      },
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cikz8pfrLRCk",
        "colab_type": "code",
        "outputId": "356184c7-1a8c-495b-d8e8-55102b3f83be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "train_data[10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTTMuvwLRCn",
        "colab_type": "text"
      },
      "source": [
        "Here's how you can decode it back to words, in case you are curious:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQCWvUshLRCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz1WfsfwLRCr",
        "colab_type": "code",
        "outputId": "d69f3397-9cf0-4d9f-feb4-4bb9fd73d108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "decoded_newswire"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvpffYEhLRCu",
        "colab_type": "text"
      },
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gty-AJ9ULRCu",
        "colab_type": "code",
        "outputId": "ad0aeda3-aa58-4230-ede4-c6bfcc6a15fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels[10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu_kYu9CLRCx",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We can vectorize the data with the exact same code as in our previous example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M9-TaXPLRCx",
        "colab_type": "code",
        "outputId": "777c9b36-3497-429e-b8b1-1fc48a7f6e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "print(x_train.shape)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8982, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_R3qvszLRC0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
        "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
        "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
        "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPdzYTP8LRC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx7ysLwLLRC4",
        "colab_type": "text"
      },
      "source": [
        "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyqSOPDbLRC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7TDu3C_LRC7",
        "colab_type": "text"
      },
      "source": [
        "## Building our network\n",
        "\n",
        "\n",
        "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
        "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
        "dimensionality of the output space is much larger. \n",
        "\n",
        "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
        "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
        "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
        "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
        "permanently dropping relevant information.\n",
        "\n",
        "For this reason we will use larger layers. Let's go with 64 units:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0v-nekXLRC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvHfa_4nLRC_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "There are two other things you should note about this architecture:\n",
        "\n",
        "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
        "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
        "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
        "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
        "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
        "\n",
        "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
        "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
        "distance between these two distributions, we train our network to output something as close as possible to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cph4ECyLRDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0y4LBRLRDC",
        "colab_type": "text"
      },
      "source": [
        "## Validating our approach\n",
        "\n",
        "Let's set apart 1,000 samples in our training data to use as a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hudGBJILRDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCVgtJ4dLRDF",
        "colab_type": "text"
      },
      "source": [
        "Now let's train our network for 20 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCD-_3nULRDG",
        "colab_type": "code",
        "outputId": "72cfa462-c2f3-4b67-8340-77f4eb5ddac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 2.6491 - accuracy: 0.5099 - val_loss: 1.7862 - val_accuracy: 0.6530\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.4418 - accuracy: 0.7081 - val_loss: 1.3181 - val_accuracy: 0.7280\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 1.0469 - accuracy: 0.7831 - val_loss: 1.1346 - val_accuracy: 0.7520\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.8260 - accuracy: 0.8270 - val_loss: 1.0254 - val_accuracy: 0.7810\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.6627 - accuracy: 0.8629 - val_loss: 0.9698 - val_accuracy: 0.7930\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 1s 62ms/step - loss: 0.5315 - accuracy: 0.8931 - val_loss: 0.9298 - val_accuracy: 0.7970\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.4293 - accuracy: 0.9142 - val_loss: 0.9058 - val_accuracy: 0.8200\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3499 - accuracy: 0.9285 - val_loss: 0.8887 - val_accuracy: 0.8100\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.2967 - accuracy: 0.9376 - val_loss: 0.8862 - val_accuracy: 0.8110\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.2449 - accuracy: 0.9455 - val_loss: 0.9130 - val_accuracy: 0.8140\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.2124 - accuracy: 0.9480 - val_loss: 0.9563 - val_accuracy: 0.8030\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1908 - accuracy: 0.9523 - val_loss: 0.9069 - val_accuracy: 0.8240\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1657 - accuracy: 0.9536 - val_loss: 0.9388 - val_accuracy: 0.8190\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.1523 - accuracy: 0.9555 - val_loss: 0.9585 - val_accuracy: 0.8140\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.1420 - accuracy: 0.9563 - val_loss: 1.0325 - val_accuracy: 0.7970\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 1s 47ms/step - loss: 0.1345 - accuracy: 0.9560 - val_loss: 0.9797 - val_accuracy: 0.8170\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1309 - accuracy: 0.9582 - val_loss: 1.0223 - val_accuracy: 0.8050\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.1188 - accuracy: 0.9560 - val_loss: 1.0310 - val_accuracy: 0.8090\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.1166 - accuracy: 0.9577 - val_loss: 1.0911 - val_accuracy: 0.8000\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.1171 - accuracy: 0.9569 - val_loss: 1.0774 - val_accuracy: 0.8060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKrrsHsSLRDI",
        "colab_type": "text"
      },
      "source": [
        "Let's display its loss and accuracy curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAkJHGlvLRDJ",
        "colab_type": "code",
        "outputId": "2953e9b3-7e3f-4c39-85a9-b10942e00bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkuOUCEREctz/B8LHKKKT22MwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlsme0BWLRDL",
        "colab_type": "code",
        "outputId": "4c2b02ec-15bd-4229-c2c7-cb61c0a38704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txNAhKE8LRDN",
        "colab_type": "text"
      },
      "source": [
        "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
        "the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRl0MSELRDN",
        "colab_type": "code",
        "outputId": "afb4ad27-e60a-452e-baa6-d8745d342948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 51ms/step - loss: 2.6434 - accuracy: 0.5183 - val_loss: 1.7035 - val_accuracy: 0.6510\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 1.3952 - accuracy: 0.7096 - val_loss: 1.2985 - val_accuracy: 0.7180\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.0449 - accuracy: 0.7742 - val_loss: 1.1252 - val_accuracy: 0.7610\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.8213 - accuracy: 0.8279 - val_loss: 1.0313 - val_accuracy: 0.7790\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.6544 - accuracy: 0.8629 - val_loss: 0.9640 - val_accuracy: 0.8060\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.5243 - accuracy: 0.8919 - val_loss: 0.9283 - val_accuracy: 0.8050\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.4190 - accuracy: 0.9147 - val_loss: 0.8870 - val_accuracy: 0.8140\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 46ms/step - loss: 0.3387 - accuracy: 0.9280 - val_loss: 0.8905 - val_accuracy: 0.8180\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 0.9978 - accuracy: 0.7841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZ9NZoQLRDP",
        "colab_type": "code",
        "outputId": "2e01309f-07c0-421d-ca92-771dedb98374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9978498816490173, 0.784060537815094]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_IioKunLRDR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
        "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDOPrkSpLRDS",
        "colab_type": "code",
        "outputId": "8f484d91-92f1-4c51-c6b7-a637ecd8a9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import copy\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18432769367764915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo2m6UEfLRDU",
        "colab_type": "text"
      },
      "source": [
        "## Generating predictions on new data\n",
        "\n",
        "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
        "predictions for all of the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6IRVPBLRDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCAQp91ZLRDX",
        "colab_type": "text"
      },
      "source": [
        "Each entry in `predictions` is a vector of length 46:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgs9Wj5fLRDY",
        "colab_type": "code",
        "outputId": "44ce1e7b-dff7-4dea-9306-ebb2dce9d313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjterCsCLRDa",
        "colab_type": "text"
      },
      "source": [
        "The coefficients in this vector sum to 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnBbTt5wLRDb",
        "colab_type": "code",
        "outputId": "6fc4acc5-b718-48ff-f92d-c95c11fc102c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5fat43HLRDd",
        "colab_type": "text"
      },
      "source": [
        "The largest entry is the predicted class, i.e. the class with the highest probability:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaB_UhZ3LRDd",
        "colab_type": "code",
        "outputId": "ae4ef2f4-d6ab-499e-9430-b5d2b413621f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6T4OAB8LRDf",
        "colab_type": "text"
      },
      "source": [
        "## A different way to handle the labels and the loss\n",
        "\n",
        "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33SjgeXxLRDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KdGZDO0LRDh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
        "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eb0P96VLRDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXb-AoQdLRDk",
        "colab_type": "text"
      },
      "source": [
        "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULvhHEQmLRDl",
        "colab_type": "text"
      },
      "source": [
        "## On the importance of having sufficiently large intermediate layers\n",
        "\n",
        "\n",
        "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
        "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
        "46-dimensional, e.g. 4-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuJutfp8LRDl",
        "colab_type": "code",
        "outputId": "8538fef2-320f-460b-d513-3582cf35dbe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "63/63 [==============================] - 1s 18ms/step - loss: 2.7115 - accuracy: 0.4206 - val_loss: 2.0143 - val_accuracy: 0.5440\n",
            "Epoch 2/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.7868 - accuracy: 0.5594 - val_loss: 1.6248 - val_accuracy: 0.5660\n",
            "Epoch 3/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.4618 - accuracy: 0.5927 - val_loss: 1.4563 - val_accuracy: 0.6050\n",
            "Epoch 4/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.2387 - accuracy: 0.6744 - val_loss: 1.3747 - val_accuracy: 0.6570\n",
            "Epoch 5/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 1.0925 - accuracy: 0.7245 - val_loss: 1.3085 - val_accuracy: 0.6790\n",
            "Epoch 6/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.9764 - accuracy: 0.7557 - val_loss: 1.2989 - val_accuracy: 0.6900\n",
            "Epoch 7/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.8810 - accuracy: 0.7880 - val_loss: 1.2823 - val_accuracy: 0.7020\n",
            "Epoch 8/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.8088 - accuracy: 0.8068 - val_loss: 1.3186 - val_accuracy: 0.7100\n",
            "Epoch 9/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.7384 - accuracy: 0.8244 - val_loss: 1.3225 - val_accuracy: 0.7110\n",
            "Epoch 10/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.6865 - accuracy: 0.8321 - val_loss: 1.3724 - val_accuracy: 0.7130\n",
            "Epoch 11/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.6344 - accuracy: 0.8363 - val_loss: 1.4008 - val_accuracy: 0.7100\n",
            "Epoch 12/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.5911 - accuracy: 0.8424 - val_loss: 1.4358 - val_accuracy: 0.7010\n",
            "Epoch 13/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.5627 - accuracy: 0.8475 - val_loss: 1.4718 - val_accuracy: 0.7070\n",
            "Epoch 14/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.5259 - accuracy: 0.8567 - val_loss: 1.5289 - val_accuracy: 0.7010\n",
            "Epoch 15/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.4988 - accuracy: 0.8667 - val_loss: 1.5394 - val_accuracy: 0.7020\n",
            "Epoch 16/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4729 - accuracy: 0.8705 - val_loss: 1.6216 - val_accuracy: 0.7020\n",
            "Epoch 17/20\n",
            "63/63 [==============================] - 1s 16ms/step - loss: 0.4523 - accuracy: 0.8768 - val_loss: 1.6580 - val_accuracy: 0.7010\n",
            "Epoch 18/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4304 - accuracy: 0.8821 - val_loss: 1.7382 - val_accuracy: 0.7040\n",
            "Epoch 19/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4100 - accuracy: 0.8842 - val_loss: 1.8096 - val_accuracy: 0.6970\n",
            "Epoch 20/20\n",
            "63/63 [==============================] - 1s 17ms/step - loss: 0.4002 - accuracy: 0.8910 - val_loss: 1.8172 - val_accuracy: 0.7020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc49013ca90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKoQDzKILRDo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
        "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
        "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
        "of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNuBr-OULRDo",
        "colab_type": "text"
      },
      "source": [
        "## Further experiments\n",
        "\n",
        "* Try using larger or smaller layers: 32 units, 128 units...\n",
        "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UgYPYkeLRDp",
        "colab_type": "text"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
        "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
        "probability distribution over the N output classes.\n",
        "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
        "probability distributions output by the network, and the true distribution of the targets.\n",
        "* There are two ways to handle labels in multi-class classification:\n",
        "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
        "function.\n",
        "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
        "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
        "intermediate layers that are too small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ObKr7f2f37",
        "colab_type": "text"
      },
      "source": [
        "# Różna ilość ukrytych wartstw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAZMLKMOQsgm",
        "colab_type": "text"
      },
      "source": [
        "Wartości porównywalne: [0.9748356938362122, 0.7853962779045105] (2 warstwy 64 elementy, relu, categorical_crossentropy, 8 epok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr-kEKQFDljT",
        "colab_type": "text"
      },
      "source": [
        "1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "095e1b14-af6e-49fa-df74-714c95d9ef29",
        "id": "TutRCLFK3LhL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 49ms/step - loss: 2.6626 - accuracy: 0.5342 - val_loss: 1.9069 - val_accuracy: 0.6490\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.5236 - accuracy: 0.7159 - val_loss: 1.3968 - val_accuracy: 0.7300\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.0894 - accuracy: 0.7892 - val_loss: 1.1468 - val_accuracy: 0.7660\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.8372 - accuracy: 0.8332 - val_loss: 1.0255 - val_accuracy: 0.7880\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.6653 - accuracy: 0.8686 - val_loss: 0.9381 - val_accuracy: 0.8010\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.5403 - accuracy: 0.8938 - val_loss: 0.8924 - val_accuracy: 0.8120\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.4491 - accuracy: 0.9094 - val_loss: 0.8469 - val_accuracy: 0.8230\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3744 - accuracy: 0.9263 - val_loss: 0.8322 - val_accuracy: 0.8160\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 0.9271 - accuracy: 0.8001\n",
            "[0.9271455407142639, 0.8000890612602234]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErBFRFA6xu7i",
        "colab_type": "code",
        "outputId": "223cd7c1-5b2f-4c53-8a5b-1d1737428d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7  2  0 ...  0  0  0]\n",
            " [ 0 86  0 ...  0  0  0]\n",
            " [ 0  3 14 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  4  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.63      0.82      0.71       105\n",
            "           2       0.82      0.70      0.76        20\n",
            "           3       0.93      0.94      0.94       813\n",
            "           4       0.80      0.91      0.85       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.66      0.66      0.66        38\n",
            "           9       0.82      0.72      0.77        25\n",
            "          10       0.86      0.83      0.85        30\n",
            "          11       0.66      0.76      0.70        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.59      0.62      0.61        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.64      0.79      0.71        99\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.71      0.60      0.65        20\n",
            "          19       0.65      0.73      0.69       133\n",
            "          20       0.64      0.56      0.60        70\n",
            "          21       0.74      0.74      0.74        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.70      0.58      0.64        12\n",
            "          24       0.60      0.32      0.41        19\n",
            "          25       0.86      0.58      0.69        31\n",
            "          26       1.00      0.75      0.86         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.33      0.10      0.15        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.88      0.58      0.70        12\n",
            "          31       0.75      0.23      0.35        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.83      0.71      0.77         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.50      0.36      0.42        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       1.00      0.20      0.33         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       1.00      0.67      0.80         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80      2246\n",
            "   macro avg       0.64      0.44      0.49      2246\n",
            "weighted avg       0.79      0.80      0.78      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL17wgX_DpPL",
        "colab_type": "text"
      },
      "source": [
        "3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohBDe7VjD8pZ",
        "colab_type": "code",
        "outputId": "7a7b4af4-3047-4cee-9cd0-82e2a1cf3844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 53ms/step - loss: 2.5660 - accuracy: 0.4739 - val_loss: 1.6666 - val_accuracy: 0.6410\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.4177 - accuracy: 0.6944 - val_loss: 1.3069 - val_accuracy: 0.7090\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 1.0890 - accuracy: 0.7526 - val_loss: 1.1723 - val_accuracy: 0.7390\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.8707 - accuracy: 0.8061 - val_loss: 1.0923 - val_accuracy: 0.7580\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.6873 - accuracy: 0.8483 - val_loss: 1.0931 - val_accuracy: 0.7470\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.5640 - accuracy: 0.8725 - val_loss: 0.9704 - val_accuracy: 0.7900\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.4532 - accuracy: 0.9024 - val_loss: 0.9814 - val_accuracy: 0.7970\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.3581 - accuracy: 0.9216 - val_loss: 0.9834 - val_accuracy: 0.8040\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 1.0919 - accuracy: 0.7720\n",
            "[1.0919252634048462, 0.7720391750335693]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b2ec09c-a57d-4d0d-db49-ef9d726de4a7",
        "id": "NSpAi-hPECYP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  2  0 ...  0  0  0]\n",
            " [ 0 89  1 ...  0  0  0]\n",
            " [ 0  2 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  2  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.42      0.56        12\n",
            "           1       0.61      0.85      0.71       105\n",
            "           2       0.80      0.60      0.69        20\n",
            "           3       0.92      0.92      0.92       813\n",
            "           4       0.80      0.90      0.85       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.92      0.86      0.89        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.67      0.63      0.65        38\n",
            "           9       0.68      0.68      0.68        25\n",
            "          10       0.79      0.73      0.76        30\n",
            "          11       0.58      0.73      0.65        83\n",
            "          12       0.20      0.08      0.11        13\n",
            "          13       0.61      0.54      0.57        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.61      0.81      0.70        99\n",
            "          17       1.00      0.08      0.15        12\n",
            "          18       0.63      0.60      0.62        20\n",
            "          19       0.58      0.77      0.66       133\n",
            "          20       0.62      0.36      0.45        70\n",
            "          21       0.57      0.78      0.66        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.56      0.42      0.48        12\n",
            "          24       0.60      0.32      0.41        19\n",
            "          25       0.82      0.58      0.68        31\n",
            "          26       0.71      0.62      0.67         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       1.00      0.10      0.18        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.50      0.17      0.25        12\n",
            "          31       1.00      0.15      0.27        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.75      0.43      0.55         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.67      0.33      0.44         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.51      0.36      0.39      2246\n",
            "weighted avg       0.76      0.77      0.75      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-rpZOu9E4zp",
        "colab_type": "text"
      },
      "source": [
        "# Warstwy z inną liczbą elementów"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmPCdl7DFNkY",
        "colab_type": "text"
      },
      "source": [
        "32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jAresoXFFvH",
        "colab_type": "code",
        "outputId": "f9450540-a097-4aaa-80fa-afc5cd0994c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 2.9414 - accuracy: 0.4837 - val_loss: 2.2320 - val_accuracy: 0.5720\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 1.8514 - accuracy: 0.6234 - val_loss: 1.6558 - val_accuracy: 0.6420\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 1.4111 - accuracy: 0.6952 - val_loss: 1.3995 - val_accuracy: 0.6960\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 1.1566 - accuracy: 0.7519 - val_loss: 1.2483 - val_accuracy: 0.7350\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.9729 - accuracy: 0.7914 - val_loss: 1.1571 - val_accuracy: 0.7500\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.8380 - accuracy: 0.8170 - val_loss: 1.0850 - val_accuracy: 0.7650\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.7197 - accuracy: 0.8441 - val_loss: 1.0386 - val_accuracy: 0.7790\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.6271 - accuracy: 0.8636 - val_loss: 1.0018 - val_accuracy: 0.7880\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 1.0639 - accuracy: 0.7680\n",
            "[1.0639104843139648, 0.7680320739746094]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuKWjnozFJuv",
        "colab_type": "code",
        "outputId": "9a5e7c16-e3dd-46c2-f2b9-dff5c7ca546a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6  3  1 ...  0  0  0]\n",
            " [ 0 91  0 ...  0  0  0]\n",
            " [ 0  5 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  2  0  0]\n",
            " [ 0  1  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.50      0.60        12\n",
            "           1       0.54      0.87      0.66       105\n",
            "           2       0.57      0.60      0.59        20\n",
            "           3       0.90      0.94      0.92       813\n",
            "           4       0.79      0.92      0.85       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       1.00      0.36      0.53        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.67      0.63      0.65        38\n",
            "           9       0.85      0.68      0.76        25\n",
            "          10       0.92      0.37      0.52        30\n",
            "          11       0.61      0.78      0.68        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.59      0.62      0.61        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.64      0.79      0.71        99\n",
            "          17       1.00      0.17      0.29        12\n",
            "          18       0.70      0.35      0.47        20\n",
            "          19       0.65      0.77      0.71       133\n",
            "          20       0.76      0.44      0.56        70\n",
            "          21       0.45      0.74      0.56        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.23      0.25      0.24        12\n",
            "          24       0.67      0.21      0.32        19\n",
            "          25       0.92      0.39      0.55        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       1.00      0.23      0.38        13\n",
            "          32       1.00      0.20      0.33        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       1.00      0.29      0.44         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       1.00      0.33      0.50         6\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.41      0.27      0.30      2246\n",
            "weighted avg       0.74      0.77      0.74      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRd9Cm9OFbbG",
        "colab_type": "text"
      },
      "source": [
        "128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4MSD0ODFc6u",
        "colab_type": "code",
        "outputId": "5ca3c90b-aee7-4b23-9ecc-bf0c1f2bf79a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 75ms/step - loss: 2.1532 - accuracy: 0.5549 - val_loss: 1.4276 - val_accuracy: 0.6730\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 1.1359 - accuracy: 0.7507 - val_loss: 1.1115 - val_accuracy: 0.7660\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 0.8013 - accuracy: 0.8222 - val_loss: 0.9979 - val_accuracy: 0.7750\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 0.5614 - accuracy: 0.8822 - val_loss: 0.9353 - val_accuracy: 0.7940\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 70ms/step - loss: 0.4196 - accuracy: 0.9116 - val_loss: 0.8667 - val_accuracy: 0.8140\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 70ms/step - loss: 0.3088 - accuracy: 0.9326 - val_loss: 0.9082 - val_accuracy: 0.8020\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 71ms/step - loss: 0.2501 - accuracy: 0.9411 - val_loss: 0.8707 - val_accuracy: 0.8240\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 69ms/step - loss: 0.2031 - accuracy: 0.9490 - val_loss: 0.9198 - val_accuracy: 0.8140\n",
            "71/71 [==============================] - 0s 3ms/step - loss: 1.0267 - accuracy: 0.7943\n",
            "[1.0266584157943726, 0.7943009734153748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bis65DqGFgGK",
        "colab_type": "code",
        "outputId": "3081fe0d-6516-4d28-bd39-f6bc4f932c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7  3  0 ...  0  0  0]\n",
            " [ 0 88  0 ...  0  0  0]\n",
            " [ 0  2 11 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  6  0  0]\n",
            " [ 0  1  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.58      0.84      0.68       105\n",
            "           2       0.79      0.55      0.65        20\n",
            "           3       0.89      0.95      0.92       813\n",
            "           4       0.85      0.85      0.85       474\n",
            "           5       1.00      0.20      0.33         5\n",
            "           6       0.86      0.86      0.86        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.75      0.63      0.69        38\n",
            "           9       0.80      0.80      0.80        25\n",
            "          10       0.89      0.83      0.86        30\n",
            "          11       0.63      0.76      0.69        83\n",
            "          12       0.60      0.23      0.33        13\n",
            "          13       0.59      0.51      0.55        37\n",
            "          14       0.50      0.50      0.50         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.67      0.79      0.72        99\n",
            "          17       0.80      0.33      0.47        12\n",
            "          18       0.63      0.60      0.62        20\n",
            "          19       0.66      0.67      0.67       133\n",
            "          20       0.63      0.56      0.59        70\n",
            "          21       0.71      0.63      0.67        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.67      0.50      0.57        12\n",
            "          24       0.53      0.42      0.47        19\n",
            "          25       0.83      0.65      0.73        31\n",
            "          26       1.00      0.75      0.86         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.50      0.10      0.17        10\n",
            "          29       0.40      0.50      0.44         4\n",
            "          30       0.83      0.42      0.56        12\n",
            "          31       1.00      0.46      0.63        13\n",
            "          32       1.00      0.80      0.89        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.67      0.57      0.62         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.62      0.45      0.53        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.67      0.40      0.50         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.50      0.25      0.33         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       0.86      1.00      0.92         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.72      0.51      0.56      2246\n",
            "weighted avg       0.79      0.79      0.78      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFPGjVeqGEjD",
        "colab_type": "text"
      },
      "source": [
        "# Zmiana funkcji straty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8bvOJnLGKFK",
        "colab_type": "text"
      },
      "source": [
        " uzycie: mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55b6a0c9-179d-4d96-fe34-c84c84ba1c4b",
        "id": "gMJFbuUtI-KM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 48ms/step - loss: 0.0170 - accuracy: 0.4835 - val_loss: 0.0122 - val_accuracy: 0.6140\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0100 - accuracy: 0.6892 - val_loss: 0.0095 - val_accuracy: 0.6930\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0082 - accuracy: 0.7260 - val_loss: 0.0088 - val_accuracy: 0.7210\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0072 - accuracy: 0.7611 - val_loss: 0.0083 - val_accuracy: 0.7340\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0064 - accuracy: 0.7874 - val_loss: 0.0079 - val_accuracy: 0.7580\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.0058 - accuracy: 0.8089 - val_loss: 0.0078 - val_accuracy: 0.7550\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0052 - accuracy: 0.8292 - val_loss: 0.0076 - val_accuracy: 0.7670\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.0047 - accuracy: 0.8533 - val_loss: 0.0073 - val_accuracy: 0.7850\n",
            "71/71 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.7582\n",
            "[0.0078001683577895164, 0.7582368850708008]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e0320527-578d-4669-928d-f6edc4533a8b",
        "id": "1YVDtfG1JCFA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6  1  0 ...  0  0  0]\n",
            " [ 0 85  0 ...  0  0  0]\n",
            " [ 1  1 12 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.50      0.55        12\n",
            "           1       0.71      0.81      0.76       105\n",
            "           2       0.55      0.60      0.57        20\n",
            "           3       0.91      0.95      0.93       813\n",
            "           4       0.87      0.86      0.86       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.20      0.71      0.31        38\n",
            "           9       0.57      0.68      0.62        25\n",
            "          10       0.72      0.87      0.79        30\n",
            "          11       0.61      0.77      0.68        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.46      0.70      0.56        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.71      0.81      0.75        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.56      0.45      0.50        20\n",
            "          19       0.60      0.75      0.67       133\n",
            "          20       0.72      0.37      0.49        70\n",
            "          21       0.45      0.85      0.59        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.00      0.00      0.00        19\n",
            "          25       0.65      0.55      0.60        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.76      2246\n",
            "   macro avg       0.24      0.26      0.24      2246\n",
            "weighted avg       0.71      0.76      0.73      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rIia9o6N7Yv",
        "colab_type": "text"
      },
      "source": [
        "#użycie funkcji aktywacji tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StGf-9E3OQzJ",
        "colab_type": "code",
        "outputId": "56e27daa-e082-4a78-80dd-4ecc19209f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='tanh', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='tanh'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "16/16 [==============================] - 1s 52ms/step - loss: 2.2781 - accuracy: 0.5515 - val_loss: 1.5890 - val_accuracy: 0.6600\n",
            "Epoch 2/8\n",
            "16/16 [==============================] - 1s 66ms/step - loss: 1.3101 - accuracy: 0.7279 - val_loss: 1.2346 - val_accuracy: 0.7490\n",
            "Epoch 3/8\n",
            "16/16 [==============================] - 1s 45ms/step - loss: 0.9777 - accuracy: 0.8038 - val_loss: 1.0606 - val_accuracy: 0.7800\n",
            "Epoch 4/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.7584 - accuracy: 0.8527 - val_loss: 0.9669 - val_accuracy: 0.7960\n",
            "Epoch 5/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.5933 - accuracy: 0.8894 - val_loss: 0.8987 - val_accuracy: 0.8090\n",
            "Epoch 6/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.4677 - accuracy: 0.9163 - val_loss: 0.8640 - val_accuracy: 0.8200\n",
            "Epoch 7/8\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.3703 - accuracy: 0.9330 - val_loss: 0.8367 - val_accuracy: 0.8240\n",
            "Epoch 8/8\n",
            "16/16 [==============================] - 1s 44ms/step - loss: 0.2979 - accuracy: 0.9424 - val_loss: 0.8435 - val_accuracy: 0.8120\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 0.9467 - accuracy: 0.7876\n",
            "[0.9466673135757446, 0.7876224517822266]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3swKDImyOVEl",
        "colab_type": "code",
        "outputId": "944aea2e-a7c4-444a-99d0-570ec19b0b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "predicted_labels=[np.argmax(prediction) for prediction in predictions ]\n",
        "\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "print(cm)\n",
        "print(classification_report(test_labels, predicted_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 7  2  0 ...  0  0  0]\n",
            " [ 0 84  0 ...  0  0  0]\n",
            " [ 0  3 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  3  0  0]\n",
            " [ 0  0  0 ...  0  4  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.63      0.80      0.71       105\n",
            "           2       0.81      0.65      0.72        20\n",
            "           3       0.90      0.94      0.92       813\n",
            "           4       0.78      0.90      0.84       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.76      0.93      0.84        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.74      0.66      0.69        38\n",
            "           9       0.85      0.68      0.76        25\n",
            "          10       0.88      0.77      0.82        30\n",
            "          11       0.61      0.73      0.67        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.64      0.62      0.63        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.50      0.11      0.18         9\n",
            "          16       0.67      0.79      0.73        99\n",
            "          17       1.00      0.08      0.15        12\n",
            "          18       0.55      0.60      0.57        20\n",
            "          19       0.65      0.68      0.67       133\n",
            "          20       0.63      0.53      0.57        70\n",
            "          21       0.72      0.67      0.69        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.62      0.42      0.50        12\n",
            "          24       0.60      0.32      0.41        19\n",
            "          25       0.83      0.61      0.70        31\n",
            "          26       1.00      0.25      0.40         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.33      0.25      0.29         4\n",
            "          30       0.67      0.50      0.57        12\n",
            "          31       0.71      0.38      0.50        13\n",
            "          32       1.00      0.60      0.75        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.80      0.57      0.67         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.50      0.09      0.15        11\n",
            "          37       0.33      0.50      0.40         2\n",
            "          38       1.00      0.33      0.50         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       1.00      0.33      0.50         3\n",
            "          43       1.00      0.50      0.67         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.79      2246\n",
            "   macro avg       0.63      0.43      0.48      2246\n",
            "weighted avg       0.77      0.79      0.77      2246\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}